---

# 'hosting_infrastructure' is used to drive the correct behavior based
# on the hosting infrastructure, cloud provider, etc. Valid values are:
# - 'openstack'
# - 'rhv'
# - 'azure' (Coming Soon)
# - 'gcp'
hosting_infrastructure: rhv

# Cluster Environment ID to uniquely identify the environment
env_id: "<REPLACE WITH VALID ENV ID - i.e: env1>"

# Define custom Master(s) API and Console Port
# Comment out these entries to use a different port for both the Web Console and OpenShift API - Default port is 8443
#openshift_master_api_port: 443
#openshift_master_console_port: 443

# Define custom console and apps public DNS Name
# NOTE: These values need to be a subset of {{ dns_domain }}
openshift_master_default_subdomain: "apps.{{ env_id }}.{{ dns_domain }}"
openshift_master_cluster_hostname: "console.internal.{{ env_id }}.{{ dns_domain }}"
openshift_master_cluster_public_hostname: "console.{{ env_id }}.{{ dns_domain }}"

# Define infrastructure skeleton
# NOTE: Flavor is your instance type configured in RHV ie 'Large'
cloud_infrastructure:
   template_name: <REPLACE WITH VALID RHV TEMPLATE>
   masters:
     count: <REPLACE WITH NUMBER OF INSTANCES TO CREATE>
     flavor: <REPLACE WITH DESIRED FLAVOR FOR THE INSTANCE>
     network: <REPLACE WITH DESIRED NETWORK FOR THE INSTANCE>
     network_netmask: <REPLACE WITH DESIRED NETWORK SUBMASK FOR THE INSTANCE>
     network_gateway: <REPLACE WITH DESIRED NETWORK GATEWAY FOR THE INSTANCE>
     network_nic_name: <REPLACE WITH DESIRED NETWORK NIC NAME FOR THE INSTANCE>
     name_prefix: <REPLACE WITH DESIRED NAME PREFIX FOR THE INSTANCE>
     rhv_cluster: <REPLACE WITH DESIRED CLUSTER>
     root_volume_size: <REPLACE WITH THE SIZE (Gi) FOR ROOT POOL DEVICE>
     rhv_storage_domain: <REPLACE WITH THE STORAGE DOMAIN TO USE FOR DOCKER STORAGE>
     docker_volume_size: <REPLACE WITH THE SIZE (Gi) FOR DOCKER POOL DEVICE>
   etcdnodes:
     count: <REPLACE WITH NUMBER OF INSTANCES TO CREATE>
     flavor: <REPLACE WITH DESIRED FLAVOR FOR THE INSTANCE>
     network: <REPLACE WITH DESIRED NETWORK FOR THE INSTANCE>
     network_netmask: <REPLACE WITH DESIRED NETWORK SUBMASK FOR THE INSTANCE>
     network_gateway: <REPLACE WITH DESIRED NETWORK GATEWAY FOR THE INSTANCE>
     network_nic_name: <REPLACE WITH DESIRED NETWORK NIC NAME FOR THE INSTANCE>
     name_prefix: <REPLACE WITH DESIRED NAME PREFIX FOR THE INSTANCE>
     rhv_cluster: <REPLACE WITH DESIRED CLUSTER>
     root_volume_size: <REPLACE WITH THE SIZE (Gi) FOR ROOT POOL DEVICE>
     rhv_storage_domain: <REPLACE WITH THE STORAGE DOMAIN TO USE FOR DOCKER STORAGE>
     docker_volume_size: <REPLACE WITH THE SIZE (Gi) FOR DOCKER POOL DEVICE>
   appnodes:
     count: <REPLACE WITH NUMBER OF INSTANCES TO CREATE>
     flavor: <REPLACE WITH DESIRED FLAVOR FOR THE INSTANCE>
     network: <REPLACE WITH DESIRED NETWORK FOR THE INSTANCE>
     network_netmask: <REPLACE WITH DESIRED NETWORK SUBMASK FOR THE INSTANCE>
     network_gateway: <REPLACE WITH DESIRED NETWORK GATEWAY FOR THE INSTANCE>
     network_nic_name: <REPLACE WITH DESIRED NETWORK NIC NAME FOR THE INSTANCE>
     name_prefix: <REPLACE WITH DESIRED NAME PREFIX FOR THE INSTANCE>
     rhv_cluster: <REPLACE WITH DESIRED CLUSTER>
     root_volume_size: <REPLACE WITH THE SIZE (Gi) FOR ROOT POOL DEVICE>
     rhv_storage_domain: <REPLACE WITH THE STORAGE DOMAIN TO USE FOR DOCKER STORAGE>
     docker_volume_size: <REPLACE WITH THE SIZE (Gi) FOR DOCKER POOL DEVICE>
   infranodes:
     count: <REPLACE WITH NUMBER OF INSTANCES TO CREATE>
     flavor: <REPLACE WITH DESIRED FLAVOR FOR THE INSTANCE>
     network: <REPLACE WITH DESIRED NETWORK FOR THE INSTANCE>
     network_netmask: <REPLACE WITH DESIRED NETWORK SUBMASK FOR THE INSTANCE>
     network_gateway: <REPLACE WITH DESIRED NETWORK GATEWAY FOR THE INSTANCE>
     network_nic_name: <REPLACE WITH DESIRED NETWORK NIC NAME FOR THE INSTANCE>
     name_prefix: <REPLACE WITH DESIRED NAME PREFIX FOR THE INSTANCE>
     rhv_cluster: <REPLACE WITH DESIRED CLUSTER>
     root_volume_size: <REPLACE WITH THE SIZE (Gi) FOR ROOT POOL DEVICE>
     rhv_storage_domain: <REPLACE WITH THE STORAGE DOMAIN TO USE FOR DOCKER STORAGE>
     docker_volume_size: <REPLACE WITH THE SIZE (Gi) FOR DOCKER POOL DEVICE>
   cnsnodes:
     count: <REPLACE WITH NUMBER OF INSTANCES TO CREATE>
     flavor: <REPLACE WITH DESIRED FLAVOR FOR THE INSTANCE>
     network: <REPLACE WITH DESIRED NETWORK FOR THE INSTANCE>
     network_netmask: <REPLACE WITH DESIRED NETWORK SUBMASK FOR THE INSTANCE>
     network_gateway: <REPLACE WITH DESIRED NETWORK GATEWAY FOR THE INSTANCE>
     network_nic_name: <REPLACE WITH DESIRED NETWORK NIC NAME FOR THE INSTANCE>
     name_prefix: <REPLACE WITH DESIRED NAME PREFIX FOR THE INSTANCE>
     rhv_cluster: <REPLACE WITH DESIRED CLUSTER>
     root_volume_size: <REPLACE WITH THE SIZE (Gi) FOR ROOT POOL DEVICE>
     rhv_storage_domain: <REPLACE WITH THE STORAGE DOMAIN TO USE FOR DOCKER STORAGE>
     docker_volume_size: <REPLACE WITH THE SIZE (Gi) FOR DOCKER POOL DEVICE>
     gluster_storage_domain: <REPLACE WITH THE STORAGE DOMAIN TO USE FOR DOCKER STORAGE>
     gluster_volume_size: <REPLACE WITH THE SIZE (Gi) FOR GLUSTER POOL DEVICE>


## Temporary set for Docker and Gluster Storage block device names
## NOTE: do not modify this info. Will be handle on a handle on a different way in the future
docker_storage_block_device: '/dev/xvdb'
docker_storage_mount_point: '/var/lib/containers/docker'
cns_node_glusterfs_volume: '/dev/xvdg'

# DNS configurations
# the 'dns_domain' will be used as the base domain for the deployment
# the 'dns_nameservers' is a list of DNS resolvers the cluster should use
dns_domain: "<REPLACE WITH A VALID DNS DOMAIN>"
dns_nameservers:
- <REPLACE WITH THE IP OF YOUR DNS SERVER>

# Specify the version of docker to use
#docker_version: "1.12.*"

# This example uses ENV variables (recommended), but the values
# can also be specified here
ovirt_url: "{{ lookup('env','OVIRT_URL') }}"
ovirt_username: "{{ lookup('env','OVIRT_USERNAME') }}"
ovirt_password: "{{ lookup('env','OVIRT_PASSWORD') }}"
ovirt_ca_file: "{{ lookup('env','OVIRT_CA') }}"

# These are the security groups created when rhv_create_vpc is 'true'. Modify accordingly to your environment in case using existing VPC and SGs
# NOTE: The use of custom VPC is not supported yet
rhv_master_sgroups: ['ocp-ssh', 'ocp-master', 'ocp-app-node']
rhv_etcd_sgroups: ['ocp-ssh', 'ocp-etcd', 'ocp-app-node']
rhv_infra_node_sgroups: ['ocp-ssh', 'ocp-infra-node', 'ocp-app-node']
rhv_app_node_sgroups: ['ocp-ssh', 'ocp-app-node']
rhv_cns_node_sgroups: ['ocp-ssh', 'ocp-app-node', 'ocp-cns']

## These are the tag used to create different dynamic groups based on these names
## NOTE: modifying these default values will affect on how the different type of nodes are discovered and configured on the required groups. You will need to update `hosts` inventory file accordingly if these values are modified
group_masters_tag: masters_rhv
group_masters_etcd_tag: masters_etcd_rhv
group_etcd_nodes_tag: etcd_nodes_rhv
group_infra_nodes_tag: infra_nodes_rhv
group_app_nodes_tag: app_nodes_rhv
group_cns_nodes_tag: cns_nodes_rhv

## These tags will define the labels your OCP Nodes will be assigned with as part of the OCP deployment process
labels_masters_tag: '{"region": "default"}'
labels_etcd_nodes_tag: '{"region": "primary"}'
labels_infra_nodes_tag: '{"region": "infra"}'
labels_app_nodes_tag: '{"region": "primary"}'
labels_cns_nodes_tag: '{"region": "primary"}'


# Subscription Management Details
rhsm_register: True
rhsm_repos:
  - "rhel-7-server-rpms"
  - "rhel-7-server-ose-3.11-rpms"
  - "rhel-7-server-extras-rpms"
  - "rhel-7-server-ansible-2.6-rpms"

# Uncomment the following to use Red Hat Satellite:
#rhsm_server_hostname: 'sat-6.example.com'
#rhsm_org_id: 'CASL_ORG'
#rhsm_activationkey: 'casl-latest'

# Uncomment the following to use RHSM username, password from environment variable:
#rhsm_username: "{{ lookup('env', 'RHSM_USER' )}}"
#rhsm_password: "{{ lookup('env', 'RHSM_PASSWD' )}}"

# leave commented out if you want to `--auto-attach` a pool
#rhsm_pool: "{{ lookup('env', 'RHSM_POOL' )}}"

# WARNING: By default the tools will update RPMs during provisioning. If any packages are
# updated, the host(s) will reboot to ensure the correct versions are in use. This may
# NOT be desirable during an consecutive runs to just apply minor changes. If you would
# like to avoid "surprise" reboots, make sure to uncomment the following variable.
# Do NOTE that a reboot should most likely happen on initial install, so it's important
# that this variable is commented out or set to `True` for initial runs.
#update_cluster_hosts: False

# Uncomment the following `additional_list_of_packages_to_install` to list additional
# packages/RPMs to install during install
#additional_list_of_packages_to_install:
#  - rpm-1
#  - rpm-2
